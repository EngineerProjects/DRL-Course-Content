<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Classic Games: CartPole / MountainCar - DRL Course</title>
    <link rel="stylesheet" href="../../static/drl-course-css.css">
    <link rel="stylesheet" href="../../static/applications.css">
    <link rel="stylesheet" href="./static/classic-control.css">
       
</head>

<body>
    <header>
        <h1>Deep Reinforcement Learning: Project</h1>
        <p>Classic Control Environments with Deep Q-Learning</p>
    </header>
    
    <nav>
        <ul>
            <li><a href="../../index.html">Home</a></li>
            <li class="dropdown">
                <a href="../Q-learning.html">Q-Learning</a>
                <ul class="submenu">
                    <li><a href="../Q-learning.html#introduction">Introduction</a></li>
                    <li><a href="../Q-learning.html#rl-basics">RL Basics</a></li>
                    <li><a href="../Q-learning.html#q-learning">Q-Learning Algorithm</a></li>
                </ul>
            </li>
            <li><a href="../deep-q-learning.html">Deep Q-Learning</a></li>
            <li><a href="../applications.html" class="active">Applications</a></li>
        </ul>
    </nav>
    
    <main>
        <section class="project-container">
            <div class="project-banner">
                <h2>Classic Games: CartPole / MountainCar</h2>
                <p>Applying Deep Q-Networks to fundamental control problems in the OpenAI Gym environment</p>
            </div>
            
            <div class="project-meta">
                <div class="meta-item">
                    <span class="meta-icon">üîç</span> Difficulty: Beginner
                </div>
                <div class="meta-item">
                    <span class="meta-icon">‚öôÔ∏è</span> Algorithm: Deep Q-Network (DQN)
                </div>
                <div class="meta-item">
                    <span class="meta-icon">‚è±Ô∏è</span> Estimated Time: 3-4 hours
                </div>
                <div class="meta-item">
                    <span class="meta-icon">üìö</span> Prerequisites: Python, Basic NN understanding, RL Fundamentals
                </div>
            </div>
            
            <h3>Project Overview</h3>
            <p>In this project, we'll implement Deep Q-Networks (DQN) to solve two classic control problems: CartPole and MountainCar. These environments serve as excellent benchmarks for testing and understanding reinforcement learning algorithms, offering different challenges that require an agent to learn complex policies from raw state observations.</p>
            
            <div class="note">
                <p><strong>Key Learning Objectives:</strong></p>
                <ul>
                    <li>Understanding how to apply Deep Q-Networks to continuous state spaces</li>
                    <li>Implementing experience replay and target networks for stable learning</li>
                    <li>Designing appropriate neural network architectures for control problems</li>
                    <li>Optimizing hyperparameters for different environment dynamics</li>
                    <li>Visualizing and interpreting agent performance and learning progress</li>
                </ul>
            </div>
            
            <h3>The Environments</h3>
            <p>We'll be working with two classic control environments from OpenAI Gym that have distinct characteristics and challenges:</p>
            
            <div class="environment-comparison">
                <div class="environment-card">
                    <div class="environment-header">
                        CartPole
                    </div>
                    <div class="environment-content">
                        <svg width="300" height="200" viewBox="0 0 300 200">
                            <!-- Sky -->
                            <rect x="0" y="0" width="300" height="150" fill="#87CEEB" />
                            
                            <!-- Ground -->
                            <rect x="0" y="150" width="300" height="50" fill="#8B4513" />
                            
                            <!-- Cart -->
                            <rect x="120" y="135" width="60" height="15" fill="#333" rx="2" />
                            
                            <!-- Wheels -->
                            <circle cx="130" cy="150" r="5" fill="#555" />
                            <circle cx="170" cy="150" r="5" fill="#555" />
                            
                            <!-- Pole -->
                            <line x1="150" y1="135" x2="150" y2="75" stroke="#FF6347" stroke-width="5" />
                            
                            <!-- Ball at top of pole -->
                            <circle cx="150" cy="75" r="7" fill="#FF6347" />
                            
                            <!-- Forces -->
                            <line x1="110" y1="142" x2="90" y2="142" stroke="#3498db" stroke-width="2" stroke-dasharray="5,3" />
                            <polygon points="90,142 95,138 95,146" fill="#3498db" />
                            <text x="90" y="135" font-size="10" fill="#3498db" text-anchor="middle">Force</text>
                        </svg>
                        <h4>Environment Description:</h4>
                        <p>A pole is attached to a cart moving along a frictionless track. The goal is to prevent the pole from falling over by moving the cart left or right.</p>
                        
                        <h4>State Space (4 dimensions):</h4>
                        <ul>
                            <li>Cart Position (x-axis)</li>
                            <li>Cart Velocity</li>
                            <li>Pole Angle</li>
                            <li>Pole Angular Velocity</li>
                        </ul>
                        
                        <h4>Action Space (2 discrete actions):</h4>
                        <ul>
                            <li>Push cart left</li>
                            <li>Push cart right</li>
                        </ul>
                        
                        <h4>Reward:</h4>
                        <ul>
                            <li>+1 for every timestep the pole remains upright</li>
                            <li>Episode ends when:
                                <ul>
                                    <li>Pole angle exceeds ¬±12 degrees</li>
                                    <li>Cart position exceeds ¬±2.4 units</li>
                                    <li>Episode length reaches 500 timesteps</li>
                                </ul>
                            </li>
                        </ul>
                        
                        <h4>Success Criteria:</h4>
                        <p>Average reward of 195.0 over 100 consecutive trials.</p>
                    </div>
                </div>
                
                <div class="environment-card">
                    <div class="environment-header">
                        MountainCar
                    </div>
                    <div class="environment-content">
                        <svg width="300" height="200" viewBox="0 0 300 200">
                            <!-- Sky -->
                            <rect x="0" y="0" width="300" height="150" fill="#87CEEB" />
                            
                            <!-- Mountain -->
                            <path d="M0,150 Q75,50 150,150 Q225,50 300,150" fill="#8B4513" />
                            
                            <!-- Car -->
                            <circle cx="95" cy="115" r="8" fill="#333" />
                            <rect x="85" y="105" width="20" height="10" fill="#ff6347" rx="3" />
                            
                            <!-- Goal flag -->
                            <line x1="225" y1="115" x2="225" y2="85" stroke="#2ecc71" stroke-width="2" />
                            <polygon points="225,85 240,90 225,95" fill="#2ecc71" />
                            
                            <!-- Forces -->
                            <line x1="75" y1="115" x2="55" y2="115" stroke="#3498db" stroke-width="2" stroke-dasharray="5,3" />
                            <polygon points="55,115 60,111 60,119" fill="#3498db" />
                            <text x="55" y="105" font-size="10" fill="#3498db" text-anchor="middle">Force</text>
                        </svg>
                        <h4>Environment Description:</h4>
                        <p>A car is positioned between two mountains. The goal is to drive up the right mountain, but the car's engine is not strong enough to climb directly. The agent must learn to build momentum by moving back and forth.</p>
                        
                        <h4>State Space (2 dimensions):</h4>
                        <ul>
                            <li>Car Position (x-axis)</li>
                            <li>Car Velocity</li>
                        </ul>
                        
                        <h4>Action Space (3 discrete actions):</h4>
                        <ul>
                            <li>Push car left</li>
                            <li>Do nothing</li>
                            <li>Push car right</li>
                        </ul>
                        
                        <h4>Reward:</h4>
                        <ul>
                            <li>-1 for each timestep until goal is reached</li>
                            <li>Episode ends when:
                                <ul>
                                    <li>Car position reaches the goal at the top of the hill (0.5 units)</li>
                                    <li>Episode length reaches 200 timesteps</li>
                                </ul>
                            </li>
                        </ul>
                        
                        <h4>Success Criteria:</h4>
                        <p>Solve the environment in less than 160 timesteps.</p>
                    </div>
                </div>
            </div>
            
            <div class="tip">
                <p>These environments are part of OpenAI Gym's Classic Control suite and can be easily accessed and used through the Gym library:</p>
                <pre>import gym
env = gym.make('CartPole-v1')  # For CartPole
env = gym.make('MountainCar-v0')  # For MountainCar</pre>
            </div>
            
            <h3>The Learning Challenge</h3>
            <p>Each environment presents unique challenges that make them ideal for learning about RL:</p>
            
            <div class="challenge-solution">
                <div class="challenge-header">CartPole Challenges</div>
                <div class="challenge-content">
                    <ul>
                        <li><strong>Balance vs. Recovery:</strong> The agent must learn both to maintain balance and recover from disturbances</li>
                        <li><strong>Delayed Consequences:</strong> The effects of actions become apparent only after several timesteps</li>
                        <li><strong>Exploration-Exploitation:</strong> The agent needs to explore enough to learn recovery strategies while exploiting known good policies</li>
                    </ul>
                </div>
            </div>
            
            <div class="challenge-solution">
                <div class="challenge-header">MountainCar Challenges</div>
                <div class="challenge-content">
                    <ul>
                        <li><strong>Sparse Rewards:</strong> The agent only receives negative rewards, making it difficult to learn progress</li>
                        <li><strong>Need for Momentum:</strong> The agent must learn a non-intuitive strategy of moving away from the goal to build momentum</li>
                        <li><strong>Exploration in Large State Space:</strong> The agent needs to explore enough to discover the momentum-building strategy</li>
                    </ul>
                </div>
            </div>
            
            <h3>Deep Q-Network Architecture</h3>
            <p>For these control problems, we'll use a relatively simple neural network architecture since the state spaces are low-dimensional:</p>
            
            <div class="network-diagram">
                <svg width="700" height="250" viewBox="0 0 700 250">
                    <!-- Input Layer -->
                    <g transform="translate(50, 20)">
                        <text x="0" y="20" font-size="16" font-weight="bold">Input Layer</text>
                        <circle cx="30" cy="60" r="20" fill="#3498db" />
                        <text x="30" y="65" font-size="12" fill="white" text-anchor="middle">x‚ÇÅ</text>
                        <circle cx="30" cy="110" r="20" fill="#3498db" />
                        <text x="30" y="115" font-size="12" fill="white" text-anchor="middle">x‚ÇÇ</text>
                        <circle cx="30" cy="160" r="20" fill="#3498db" />
                        <text x="30" y="165" font-size="12" fill="white" text-anchor="middle">x‚ÇÉ</text>
                        <circle cx="30" cy="210" r="20" fill="#3498db" />
                        <text x="30" y="215" font-size="12" fill="white" text-anchor="middle">x‚ÇÑ</text>
                        <text x="30" y="250" font-size="12" text-anchor="middle">State Inputs</text>
                    </g>
                    
                    <!-- Hidden Layer 1 -->
                    <g transform="translate(200, 20)">
                        <text x="0" y="20" font-size="16" font-weight="bold">Hidden Layer 1</text>
                        <circle cx="30" cy="60" r="20" fill="#9b59b6" />
                        <text x="30" y="65" font-size="12" fill="white" text-anchor="middle">h‚ÇÅ</text>
                        <circle cx="30" cy="110" r="20" fill="#9b59b6" />
                        <text x="30" y="115" font-size="12" fill="white" text-anchor="middle">h‚ÇÇ</text>
                        <circle cx="30" cy="160" r="20" fill="#9b59b6" />
                        <text x="30" y="165" font-size="12" fill="white" text-anchor="middle">h‚ÇÉ</text>
                        <circle cx="30" cy="210" r="20" fill="#9b59b6" />
                        <text x="30" y="215" font-size="12" fill="white" text-anchor="middle">h‚ÇÑ</text>
                        <text x="30" y="250" font-size="12" text-anchor="middle">64 Units</text>
                        <text x="30" y="270" font-size="12" text-anchor="middle">ReLU</text>
                    </g>
                    
                    <!-- Hidden Layer 2 -->
                    <g transform="translate(350, 20)">
                        <text x="0" y="20" font-size="16" font-weight="bold">Hidden Layer 2</text>
                        <circle cx="30" cy="60" r="20" fill="#9b59b6" />
                        <text x="30" y="65" font-size="12" fill="white" text-anchor="middle">h‚ÇÅ</text>
                        <circle cx="30" cy="110" r="20" fill="#9b59b6" />
                        <text x="30" y="115" font-size="12" fill="white" text-anchor="middle">h‚ÇÇ</text>
                        <circle cx="30" cy="160" r="20" fill="#9b59b6" />
                        <text x="30" y="165" font-size="12" fill="white" text-anchor="middle">h‚ÇÉ</text>
                        <circle cx="30" cy="210" r="20" fill="#9b59b6" />
                        <text x="30" y="215" font-size="12" fill="white" text-anchor="middle">h‚ÇÑ</text>
                        <text x="30" y="250" font-size="12" text-anchor="middle">32 Units</text>
                        <text x="30" y="270" font-size="12" text-anchor="middle">ReLU</text>
                    </g>
                    
                    <!-- Output Layer -->
                    <g transform="translate(500, 20)">
                        <text x="0" y="20" font-size="16" font-weight="bold">Output Layer</text>
                        <circle cx="30" cy="110" r="20" fill="#2ecc71" />
                        <text x="30" y="115" font-size="12" fill="white" text-anchor="middle">Q‚ÇÅ</text>
                        <circle cx="30" cy="160" r="20" fill="#2ecc71" />
                        <text x="30" y="165" font-size="12" fill="white" text-anchor="middle">Q‚ÇÇ</text>
                        <text x="30" y="200" font-size="12" text-anchor="middle">n Units</text>
                        <text x="30" y="220" font-size="12" text-anchor="middle">(n = number of actions)</text>
                        <text x="30" y="240" font-size="12" text-anchor="middle">Linear</text>
                    </g>
                    
                    <!-- Connections - just a sample of connections for visual effect -->
                    <!-- Input to Hidden 1 -->
                    <g stroke="#ccc" stroke-width="1">
                        <line x1="100" y1="60" x2="200" y2="60" />
                        <line x1="100" y1="60" x2="200" y2="110" />
                        <line x1="100" y1="60" x2="200" y2="160" />
                        <line x1="100" y1="110" x2="200" y2="60" />
                        <line x1="100" y1="110" x2="200" y2="110" />
                        <line x1="100" y1="110" x2="200" y2="160" />
                        <line x1="100" y1="160" x2="200" y2="110" />
                        <line x1="100" y1="160" x2="200" y2="160" />
                        <line x1="100" y1="160" x2="200" y2="210" />
                        <line x1="100" y1="210" x2="200" y2="160" />
                        <line x1="100" y1="210" x2="200" y2="210" />
                    </g>
                    
                    <!-- Hidden 1 to Hidden 2 -->
                    <g stroke="#ccc" stroke-width="1">
                        <line x1="250" y1="60" x2="350" y2="60" />
                        <line x1="250" y1="60" x2="350" y2="110" />
                        <line x1="250" y1="110" x2="350" y2="60" />
                        <line x1="250" y1="110" x2="350" y2="110" />
                        <line x1="250" y1="110" x2="350" y2="160" />
                        <line x1="250" y1="160" x2="350" y2="110" />
                        <line x1="250" y1="160" x2="350" y2="160" />
                        <line x1="250" y1="160" x2="350" y2="210" />
                        <line x1="250" y1="210" x2="350" y2="160" />
                        <line x1="250" y1="210" x2="350" y2="210" />
                    </g>
                    
                    <!-- Hidden 2 to Output -->
                    <g stroke="#ccc" stroke-width="1">
                        <line x1="400" y1="60" x2="500" y2="110" />
                        <line x1="400" y1="110" x2="500" y2="110" />
                        <line x1="400" y1="110" x2="500" y2="160" />
                        <line x1="400" y1="160" x2="500" y2="110" />
                        <line x1="400" y1="160" x2="500" y2="160" />
                        <line x1="400" y1="210" x2="500" y2="160" />
                    </g>
                </svg>
                <p><em>Figure 1: Neural network architecture for the DQN agent. For CartPole, the output layer has 2 units; for MountainCar, it has 3 units.</em></p>
            </div>
            
            <p>The key characteristics of this architecture are:</p>
            <ul>
                <li><strong>Input Layer:</strong> Matches the state dimension (4 for CartPole, 2 for MountainCar)</li>
                <li><strong>Hidden Layers:</strong> Two fully connected layers with ReLU activations</li>
                <li><strong>Output Layer:</strong> One neuron per action (2 for CartPole, 3 for MountainCar) with linear activation</li>
            </ul>
            
            <div class="note">
                <p>Since these environments have low-dimensional state spaces, we use a simple fully-connected neural network. For environments with higher-dimensional states (like images), convolutional layers would be more appropriate.</p>
            </div>
            
            <h3>Implementation Approach</h3>
            <p>We'll implement the DQN algorithm with the key components that make it stable and efficient:</p>
            
            <div class="tabs">
                <button class="tab active" onclick="openTab(event, 'dqn-components')">Key Components</button>
                <button class="tab" onclick="openTab(event, 'network-architecture')">Network Architecture</button>
                <button class="tab" onclick="openTab(event, 'hyperparameters')">Hyperparameters</button>
            </div>
            
            <div id="dqn-components" class="tab-content active">
                <h4>1. Experience Replay</h4>
                <p>Stores agent experiences to break correlations in the training data and makes more efficient use of observed data:</p>
                <ul>
                    <li>Buffer size: 10,000 experiences</li>
                    <li>Each experience is a tuple: (state, action, reward, next_state, done)</li>
                    <li>During training, randomly sample mini-batches to update the network</li>
                </ul>
                
                <h4>2. Target Network</h4>
                <p>Separate network used for generating target values to stabilize training:</p>
                <ul>
                    <li>Main network: Updated every step with gradient descent</li>
                    <li>Target network: Periodically updated to match main network</li>
                    <li>Update frequency: Every 100 steps</li>
                </ul>
                
                <h4>3. Epsilon-Greedy Exploration</h4>
                <p>Balances exploration and exploitation:</p>
                <ul>
                    <li>Starting Œµ: 1.0 (100% random actions)</li>
                    <li>Ending Œµ: 0.01 (1% random actions)</li>
                    <li>Decay rate: 0.995 per episode</li>
                </ul>
                
                <h4>4. RL Algorithm Structure</h4>
                <p>The overall structure follows these steps:</p>
                <ol>
                    <li>Initialize replay memory, main network, and target network</li>
                    <li>For each episode:
                        <ul>
                            <li>Reset environment, get initial state</li>
                            <li>For each timestep:
                                <ul>
                                    <li>Select action using epsilon-greedy policy</li>
                                    <li>Take action, observe reward and next state</li>
                                    <li>Store experience in replay memory</li>
                                    <li>Sample random batch from replay memory</li>
                                    <li>Compute target Q-values using target network</li>
                                    <li>Update main network with gradient descent</li>
                                    <li>Periodically update target network</li>
                                </ul>
                            </li>
                            <li>Decay exploration rate</li>
                            <li>Record episode performance metrics</li>
                        </ul>
                    </li>
                </ol>
            </div>
            
            <div id="network-architecture" class="tab-content">
                <h4>Network Architecture Details</h4>
                <p>We'll implement the neural network using PyTorch with the following structure:</p>
                
                <div class="code-concept">
<pre style="background-color: #1e1e1e; color: #d4d4d4; padding: 15px; border-radius: 5px; overflow-x: auto;">
<span style="color: #569cd6;">class</span> <span style="color: #4ec9b0;">DQN</span>(nn.Module):
    <span style="color: #569cd6;">def</span> <span style="color: #dcdcaa;">__init__</span>(self, state_size, action_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_size, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, action_size)
    
    <span style="color: #569cd6;">def</span> <span style="color: #dcdcaa;">forward</span>(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        return self.fc3(x)  <span style="color: #6a9955;">// Q-values for each action</span>
</pre>
                </div>
                
                <h4>Network Design Considerations</h4>
                <ul>
                    <li><strong>Layer sizes:</strong> The network is relatively small due to the simple nature of the environment states. For more complex environments, deeper and wider networks might be necessary.</li>
                    <li><strong>Activation functions:</strong> ReLU activations are used for hidden layers to introduce non-linearity while avoiding vanishing gradient problems.</li>
                    <li><strong>Output layer:</strong> Linear activation for the output layer, as Q-values can be any real number.</li>
                    <li><strong>State preprocessing:</strong> Minimal preprocessing is needed as the state variables are already normalized in most Gym environments. However, for stability, we might want to normalize or clip state values.</li>
                </ul>
                
                <h4>Different Networks for Different Environments</h4>
                <p>The network architecture is flexible and can be adjusted based on the specific environment:</p>
                <ul>
                    <li><strong>CartPole:</strong> Input layer accepts 4 state variables</li>
                    <li><strong>MountainCar:</strong> Input layer accepts 2 state variables</li>
                </ul>
                <p>The output layer size is determined by the number of possible actions in each environment (2 for CartPole, 3 for MountainCar).</p>
            </div>
            
            <div id="hyperparameters" class="tab-content">
                <h4>DQN Hyperparameters</h4>
                <p>Hyperparameters significantly affect learning performance. Here are the recommended values for each environment:</p>
                
                <div class="comparison-table">
                    <table>
                        <thead>
                            <tr>
                                <th>Hyperparameter</th>
                                <th>CartPole Value</th>
                                <th>MountainCar Value</th>
                                <th>Description</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Learning Rate</td>
                                <td>0.001</td>
                                <td>0.001</td>
                                <td>Step size for gradient updates</td>
                            </tr>
                            <tr>
                                <td>Discount Factor (Œ≥)</td>
                                <td>0.99</td>
                                <td>0.99</td>
                                <td>Weight given to future rewards</td>
                            </tr>
                            <tr>
                                <td>Epsilon Start</td>
                                <td>1.0</td>
                                <td>1.0</td>
                                <td>Initial exploration rate</td>
                            </tr>
                            <tr>
                                <td>Epsilon End</td>
                                <td>0.01</td>
                                <td>0.01</td>
                                <td>Final exploration rate</td>
                            </tr>
                            <tr>
                                <td>Epsilon Decay</td>
                                <td>0.995</td>
                                <td>0.995</td>
                                <td>Rate at which exploration decreases</td>
                            </tr>
                            <tr>
                                <td>Replay Buffer Size</td>
                                <td>10,000</td>
                                <td>10,000</td>
                                <td>Number of experiences to store</td>
                            </tr>
                            <tr>
                                <td>Batch Size</td>
                                <td>64</td>
                                <td>64</td>
                                <td>Number of experiences to sample for updates</td>
                            </tr>
                            <tr>
                                <td>Target Update Frequency</td>
                                <td>100 steps</td>
                                <td>100 steps</td>
                                <td>How often to update target network</td>
                            </tr>
                            <tr>
                                <td>Training Episodes</td>
                                <td>500</td>
                                <td>1000</td>
                                <td>Number of episodes for training</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                
                <h4>Hyperparameter Tuning Tips</h4>
                <p>When adapting these hyperparameters for your own implementation, consider these tips:</p>
                <ul>
                    <li><strong>Learning Rate:</strong> If learning is unstable, try a smaller learning rate (e.g., 0.0005). If learning is too slow, try a larger one.</li>
                    <li><strong>Epsilon Decay:</strong> Adjust based on environment complexity. Slower decay allows more exploration.</li>
                    <li><strong>Replay Buffer Size:</strong> Larger buffers provide more diverse experiences but require more memory.</li>
                    <li><strong>Target Update Frequency:</strong> More frequent updates lead to faster learning but may reduce stability.</li>
                </ul>
            </div>
            
            <h3>Algorithm Implementation</h3>
            <p>Here's the pseudocode for implementing a DQN agent to solve the CartPole and MountainCar environments:</p>
            
            <div class="algorithm-explanation">
                <h4>DQN Algorithm Pseudocode</h4>
                <div class="code-concept">
<pre style="background-color: #1e1e1e; color: #d4d4d4; padding: 15px; border-radius: 5px; overflow-x: auto;">
<span style="color: #569cd6;">Algorithm:</span> Deep Q-Network for Classic Control Environments

<span style="color: #569cd6;">// Initialize</span>
Initialize replay memory buffer with capacity N
Initialize action-value function Q with random weights Œ∏
Initialize target action-value function QÃÇ with weights Œ∏‚Åª = Œ∏
Initialize environment

<span style="color: #569cd6;">// Parameters</span>
learning_rate Œ± = 0.001
discount_factor Œ≥ = 0.99
exploration_rate Œµ = 1.0
exploration_min = 0.01
exploration_decay = 0.995
batch_size = 64
target_update_frequency = 100

<span style="color: #569cd6;">// Training loop</span>
For episode = 1 to MAX_EPISODES:
    Reset environment, observe initial state s‚ÇÅ
    For t = 1 to MAX_STEPS:
        With probability Œµ select a random action a‚Çú
        Otherwise select a‚Çú = argmax_a Q(s‚Çú, a; Œ∏)
        
        Execute action a‚Çú in the environment
        Observe reward r‚Çú and next state s‚Çú‚Çä‚ÇÅ
        Store transition (s‚Çú, a‚Çú, r‚Çú, s‚Çú‚Çä‚ÇÅ, done) in replay memory
        
        <span style="color: #569cd6;">// Experience replay</span>
        If replay memory contains enough samples:
            Sample random minibatch of transitions from replay memory
            For each transition (s‚±º, a‚±º, r‚±º, s‚±º‚Çä‚ÇÅ, done‚±º) in minibatch:
                If done‚±º:
                    Set target y‚±º = r‚±º
                Else:
                    Set target y‚±º = r‚±º + Œ≥ * max_a' QÃÇ(s‚±º‚Çä‚ÇÅ, a'; Œ∏‚Åª)
            
            Perform gradient descent step on (y‚±º - Q(s‚±º, a‚±º; Œ∏))¬≤ with respect to Œ∏
        
        <span style="color: #569cd6;">// Update target network</span>
        Every target_update_frequency steps:
            Set Œ∏‚Åª = Œ∏
        
        If done‚Çú:
            Break
    
    <span style="color: #569cd6;">// Update exploration rate</span>
    Œµ = max(exploration_min, Œµ * exploration_decay)
    
    <span style="color: #569cd6;">// Log performance</span>
    Record episode reward, steps, and other metrics
    If solved (performance meets success criteria):
        Break

<span style="color: #569cd6;">// Testing</span>
For episode = 1 to TEST_EPISODES:
    Reset environment, observe initial state s‚ÇÅ
    For t = 1 to MAX_STEPS:
        Select a‚Çú = argmax_a Q(s‚Çú, a; Œ∏) <span style="color: #6a9955;">// No exploration during testing</span>
        Execute action a‚Çú in the environment
        Observe reward r‚Çú and next state s‚Çú‚Çä‚ÇÅ
        Render environment if visualization is enabled
        If done‚Çú:
            Break
    Record test performance metrics
</pre>
                </div>
            </div>
            
            <h3>Specialized Approaches for Each Environment</h3>
            <p>While the core DQN algorithm remains the same, each environment benefits from specific optimizations:</p>
            
            <div class="challenge-solution">
                <div class="challenge-header">CartPole-Specific Approach</div>
                <div class="challenge-content">
                    <p>For CartPole, the reward structure is already well-designed (+1 for each timestep), but we can improve learning with these techniques:</p>
                    <ul>
                        <li><strong>Early Termination Penalty:</strong> Add a negative reward (-10) when the episode terminates early to discourage failure</li>
                        <li><strong>Frame Skipping:</strong> For more efficient learning, you can take the same action for 2-4 consecutive frames</li>
                        <li><strong>Normalized States:</strong> Although the environment states are already within reasonable ranges, normalizing them can improve learning stability</li>
                    </ul>
                </div>
                <div class="solution-header">Implementation Tips</div>
                <div class="solution-content">
                    <ul>
                        <li><strong>Success Definition:</strong> CartPole is considered "solved" when the average reward over 100 consecutive episodes is at least 195.0</li>
                        <li><strong>Training Duration:</strong> With the hyperparameters provided, CartPole typically requires 200-300 episodes to reach the solution criteria</li>
                        <li><strong>Visualization:</strong> Plotting the episode length over time is a useful way to track progress</li>
                    </ul>
                </div>
            </div>
            
            <div class="challenge-solution">
                <div class="challenge-header">MountainCar-Specific Approach</div>
                <div class="challenge-content">
                    <p>MountainCar has a sparse reward structure (-1 for each step) which makes learning challenging. These modifications can help:</p>
                    <ul>
                        <li><strong>Reward Shaping:</strong> Add a shaped reward based on position and velocity to provide more learning signal:
                            <pre>shaped_reward = -1.0 + abs(next_state[0] - (-0.5))</pre>
                            This rewards the agent for moving away from the starting position (-0.5)
                        </li>
                        <li><strong>Prioritized Experience Replay:</strong> Prioritize experiences with higher TD error to learn more efficiently from rare successful states</li>
                        <li><strong>Longer Training:</strong> MountainCar typically requires more episodes to solve due to the exploration challenge</li>
                    </ul>
                </div>
                <div class="solution-header">Implementation Tips</div>
                <div class="solution-content">
                    <ul>
                        <li><strong>Success Definition:</strong> MountainCar is considered "solved" when the average reward over 100 consecutive episodes is greater than -110.0</li>
                        <li><strong>Training Duration:</strong> Expect 500-1000 episodes with reward shaping, or significantly more without it</li>
                        <li><strong>Exploration Strategy:</strong> Consider using a higher exploration rate and slower decay for this environment</li>
                    </ul>
                </div>
            </div>
            
            <h3>Expected Results</h3>
            <p>After successful training, your agents should demonstrate the following behaviors:</p>
            
            <div class="environment-comparison">
                <div class="environment-card">
                    <div class="environment-header">
                        CartPole Results
                    </div>
                    <div class="environment-content">
                        <ul>
                            <li>The agent learns to balance the pole by making small, corrective movements</li>
                            <li>Episodes consistently last for the maximum duration (500 steps)</li>
                            <li>The agent can recover from moderate disturbances</li>
                            <li>Learning curve shows steady improvement, typically solving the environment within 200-300 episodes</li>
                        </ul>
                        
                        <div style="text-align: center; margin-top: 1rem;">
                            <svg width="300" height="200" viewBox="0 0 300 200">
                                <!-- Background and Axes -->
                                <rect x="40" y="20" width="240" height="160" fill="#f8f9fa" stroke="#ccc" />
                                <line x1="40" y1="180" x2="280" y2="180" stroke="#333" stroke-width="2" />
                                <line x1="40" y1="180" x2="40" y2="20" stroke="#333" stroke-width="2" />
                                
                                <!-- Axis Labels -->
                                <text x="160" y="198" font-size="12" text-anchor="middle">Episodes</text>
                                <text x="15" y="100" font-size="12" text-anchor="middle" transform="rotate(-90, 15, 100)">Episode Length</text>
                                
                                <!-- Ticks -->
                                <line x1="40" y1="180" x2="40" y2="184" stroke="#333" stroke-width="1" />
                                <text x="40" y="195" font-size="10" text-anchor="middle">0</text>
                                
                                <line x1="100" y1="180" x2="100" y2="184" stroke="#333" stroke-width="1" />
                                <text x="100" y="195" font-size="10" text-anchor="middle">100</text>
                                
                                <line x1="160" y1="180" x2="160" y2="184" stroke="#333" stroke-width="1" />
                                <text x="160" y="195" font-size="10" text-anchor="middle">200</text>
                                
                                <line x1="220" y1="180" x2="220" y2="184" stroke="#333" stroke-width="1" />
                                <text x="220" y="195" font-size="10" text-anchor="middle">300</text>
                                
                                <line x1="280" y1="180" x2="280" y2="184" stroke="#333" stroke-width="1" />
                                <text x="280" y="195" font-size="10" text-anchor="middle">400</text>
                                
                                <line x1="36" y1="180" x2="40" y2="180" stroke="#333" stroke-width="1" />
                                <text x="32" y="184" font-size="10" text-anchor="end">0</text>
                                
                                <line x1="36" y1="140" x2="40" y2="140" stroke="#333" stroke-width="1" />
                                <text x="32" y="144" font-size="10" text-anchor="end">100</text>
                                
                                <line x1="36" y1="100" x2="40" y2="100" stroke="#333" stroke-width="1" />
                                <text x="32" y="104" font-size="10" text-anchor="end">200</text>
                                
                                <line x1="36" y1="60" x2="40" y2="60" stroke="#333" stroke-width="1" />
                                <text x="32" y="64" font-size="10" text-anchor="end">300</text>
                                
                                <line x1="36" y1="20" x2="40" y2="20" stroke="#333" stroke-width="1" />
                                <text x="32" y="24" font-size="10" text-anchor="end">500</text>
                                
                                <!-- Data line -->
                                <polyline points="
                                    40,170 
                                    60,160 
                                    80,140 
                                    100,120 
                                    120,90 
                                    140,70 
                                    160,50 
                                    180,30 
                                    200,25 
                                    220,22 
                                    240,20 
                                    260,20 
                                    280,20
                                " fill="none" stroke="#3498db" stroke-width="2" />
                                
                                <!-- Moving average line -->
                                <polyline points="
                                    40,170 
                                    60,165 
                                    80,150 
                                    100,130 
                                    120,110 
                                    140,80 
                                    160,60 
                                    180,40 
                                    200,30 
                                    220,25 
                                    240,22 
                                    260,20 
                                    280,20
                                " fill="none" stroke="#e74c3c" stroke-width="2" stroke-dasharray="5,3" />
                                
                                <!-- Legend -->
                                <rect x="160" y="30" width="110" height="40" fill="white" stroke="#ccc" />
                                <line x1="170" y1="45" x2="190" y2="45" stroke="#3498db" stroke-width="2" />
                                <text x="195" y="48" font-size="10">Episode Length</text>
                                <line x1="170" y1="65" x2="190" y2="65" stroke="#e74c3c" stroke-width="2" stroke-dasharray="5,3" />
                                <text x="195" y="68" font-size="10">Moving Average</text>
                            </svg>
                            <p><em>Figure 2: Typical learning curve for CartPole showing episode length over training</em></p>
                        </div>
                    </div>
                </div>
                
                <div class="environment-card">
                    <div class="environment-header">
                        MountainCar Results
                    </div>
                    <div class="environment-content">
                        <ul>
                            <li>The agent learns to build momentum by moving back and forth</li>
                            <li>Episodes become shorter as the agent learns more efficient strategies</li>
                            <li>The car successfully reaches the flag at the top of the hill</li>
                            <li>Learning curve shows gradual improvement, with solution typically achieved after 500-1000 episodes (with reward shaping)</li>
                        </ul>
                        
                        <div style="text-align: center; margin-top: 1rem;">
                            <svg width="300" height="200" viewBox="0 0 300 200">
                                <!-- Background and Axes -->
                                <rect x="40" y="20" width="240" height="160" fill="#f8f9fa" stroke="#ccc" />
                                <line x1="40" y1="180" x2="280" y2="180" stroke="#333" stroke-width="2" />
                                <line x1="40" y1="180" x2="40" y2="20" stroke="#333" stroke-width="2" />
                                
                                <!-- Axis Labels -->
                                <text x="160" y="198" font-size="12" text-anchor="middle">Episodes</text>
                                <text x="15" y="100" font-size="12" text-anchor="middle" transform="rotate(-90, 15, 100)">Reward</text>
                                
                                <!-- Ticks -->
                                <line x1="40" y1="180" x2="40" y2="184" stroke="#333" stroke-width="1" />
                                <text x="40" y="195" font-size="10" text-anchor="middle">0</text>
                                
                                <line x1="100" y1="180" x2="100" y2="184" stroke="#333" stroke-width="1" />
                                <text x="100" y="195" font-size="10" text-anchor="middle">250</text>
                                
                                <line x1="160" y1="180" x2="160" y2="184" stroke="#333" stroke-width="1" />
                                <text x="160" y="195" font-size="10" text-anchor="middle">500</text>
                                
                                <line x1="220" y1="180" x2="220" y2="184" stroke="#333" stroke-width="1" />
                                <text x="220" y="195" font-size="10" text-anchor="middle">750</text>
                                
                                <line x1="280" y1="180" x2="280" y2="184" stroke="#333" stroke-width="1" />
                                <text x="280" y="195" font-size="10" text-anchor="middle">1000</text>
                                
                                <line x1="36" y1="180" x2="40" y2="180" stroke="#333" stroke-width="1" />
                                <text x="32" y="184" font-size="10" text-anchor="end">-200</text>
                                
                                <line x1="36" y1="140" x2="40" y2="140" stroke="#333" stroke-width="1" />
                                <text x="32" y="144" font-size="10" text-anchor="end">-180</text>
                                
                                <line x1="36" y1="100" x2="40" y2="100" stroke="#333" stroke-width="1" />
                                <text x="32" y="104" font-size="10" text-anchor="end">-160</text>
                                
                                <line x1="36" y1="60" x2="40" y2="60" stroke="#333" stroke-width="1" />
                                <text x="32" y="64" font-size="10" text-anchor="end">-140</text>
                                
                                <line x1="36" y1="20" x2="40" y2="20" stroke="#333" stroke-width="1" />
                                <text x="32" y="24" font-size="10" text-anchor="end">-120</text>
                                
                                <!-- Data line -->
                                <polyline points="
                                    40,180 
                                    60,180 
                                    80,175 
                                    100,170 
                                    120,170 
                                    140,165 
                                    160,160 
                                    180,150 
                                    200,140 
                                    220,120 
                                    240,90 
                                    260,50 
                                    280,30
                                " fill="none" stroke="#3498db" stroke-width="2" />
                                
                                <!-- Moving average line -->
                                <polyline points="
                                    40,180 
                                    60,180 
                                    80,178 
                                    100,175 
                                    120,172 
                                    140,168 
                                    160,164 
                                    180,158 
                                    200,150 
                                    220,140 
                                    240,120 
                                    260,80 
                                    280,40
                                " fill="none" stroke="#e74c3c" stroke-width="2" stroke-dasharray="5,3" />
                                
                                <!-- Legend -->
                                <rect x="160" y="30" width="110" height="40" fill="white" stroke="#ccc" />
                                <line x1="170" y1="45" x2="190" y2="45" stroke="#3498db" stroke-width="2" />
                                <text x="195" y="48" font-size="10">Episode Reward</text>
                                <line x1="170" y1="65" x2="190" y2="65" stroke="#e74c3c" stroke-width="2" stroke-dasharray="5,3" />
                                <text x="195" y="68" font-size="10">Moving Average</text>
                            </svg>
                            <p><em>Figure 3: Typical learning curve for MountainCar showing reward improvement over training</em></p>
                        </div>
                    </div>
                </div>
            </div>
            
            <h3>Common Challenges and Solutions</h3>
            
            <div class="implementation-steps">
                <div class="step">
                    <h4>Unstable Learning</h4>
                    <p><strong>Challenge:</strong> The agent's performance fluctuates dramatically during training, with periods of good performance followed by regression.</p>
                    <p><strong>Solutions:</strong></p>
                    <ul>
                        <li>Reduce the learning rate to make updates more stable</li>
                        <li>Increase the replay buffer size to provide more diverse experiences</li>
                        <li>Update the target network more frequently</li>
                        <li>Implement gradient clipping to prevent large parameter updates</li>
                        <li>Add L2 regularization to prevent overfitting</li>
                    </ul>
                </div>
                
                <div class="step">
                    <h4>Slow Learning in MountainCar</h4>
                    <p><strong>Challenge:</strong> The agent struggles to learn an effective strategy in MountainCar due to sparse rewards.</p>
                    <p><strong>Solutions:</strong></p>
                    <ul>
                        <li>Implement reward shaping as described earlier</li>
                        <li>Use prioritized experience replay to learn more efficiently from important transitions</li>
                        <li>Implement a more sophisticated exploration strategy, such as Noisy Networks or parameter space noise</li>
                        <li>Pre-train the agent with demonstrations (if available) using imitation learning</li>
                    </ul>
                </div>
                
                <div class="step">
                    <h4>Catastrophic Forgetting</h4>
                    <p><strong>Challenge:</strong> The agent suddenly loses previously learned skills, particularly in CartPole where it had learned to balance well.</p>
                    <p><strong>Solutions:</strong></p>
                    <ul>
                        <li>Decrease the learning rate as performance improves</li>
                        <li>Increase the replay buffer size to retain more past experiences</li>
                        <li>Implement experience replay with higher probability of sampling successful episodes</li>
                        <li>Save model checkpoints at various stages of training</li>
                    </ul>
                </div>
                
                <div class="step">
                    <h4>Poor Generalization</h4>
                    <p><strong>Challenge:</strong> The agent performs well during training but fails to generalize to slightly different scenarios or when started from different initial positions.</p>
                    <p><strong>Solutions:</strong></p>
                    <ul>
                        <li>Add noise to the observations during training to increase robustness</li>
                        <li>Implement domain randomization by varying environment parameters</li>
                        <li>Train the agent with a curriculum of increasingly challenging scenarios</li>
                        <li>Use a larger network or add dropout layers to prevent overfitting</li>
                    </ul>
                </div>
            </div>
            
            <h3>Complete Implementation</h3>
            <p>For the complete implementation of this project with all the optimizations described, check out our GitHub repository. The code includes:</p>
            <ul>
                <li>Full DQN implementation for both CartPole and MountainCar</li>
                <li>Custom reward shaping and environment wrappers</li>
                <li>Visualization tools to track learning progress</li>
                <li>Experimentation framework for hyperparameter tuning</li>
                <li>Saved model weights for pre-trained agents</li>
            </ul>
            
            <a href="https://github.com/EngineerProjects/Deep-Reinforcement-Learning/tree/main/02_Classic_Control" target="_blank" class="github-link">
                <svg height="20" viewBox="0 0 16 16" width="20" fill="currentColor">
                    <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
                View Project on GitHub
            </a>
            
            <h3>Extensions and Further Exploration</h3>
            <p>Once you've successfully implemented DQN for these classic control problems, consider these extensions to deepen your understanding:</p>
            
            <div class="note">
                <p><strong>Potential Extensions:</strong></p>
                <ul>
                    <li><strong>Advanced DQN Variants:</strong> Implement improvements like Double DQN, Dueling DQN, or Rainbow DQN</li>
                    <li><strong>Policy Gradient Methods:</strong> Implement REINFORCE or Advantage Actor-Critic for these environments and compare performance</li>
                    <li><strong>Continuous Control:</strong> Modify the environments to have continuous action spaces and implement algorithms like DDPG</li>
                    <li><strong>Multi-Agent Scenarios:</strong> Extend the environments to include multiple interacting agents</li>
                    <li><strong>Model-Based RL:</strong> Implement a model of the environment dynamics and use it for planning</li>
                    <li><strong>Transfer Learning:</strong> Train an agent on one environment and fine-tune it on another</li>
                </ul>
            </div>
            
            <h3>Conclusion</h3>
            <p>The CartPole and MountainCar environments provide excellent starting points for understanding and implementing Deep Q-Networks. Through this project, you've learned how to:</p>
            <ul>
                <li>Apply deep reinforcement learning to continuous state spaces</li>
                <li>Implement the key components of DQN that enable stable learning</li>
                <li>Handle different types of learning challenges (balance vs. momentum building)</li>
                <li>Optimize agent performance through hyperparameter tuning and reward design</li>
                <li>Visualize and interpret learning progress</li>
            </ul>
            
            <p>These skills form a solid foundation for tackling more complex reinforcement learning problems and implementing advanced algorithms. As you move forward, you'll find that many of the concepts and techniques used here are applicable to a wide range of reinforcement learning challenges.</p>
        </section>
    </main>
    
    <footer>
        <p>&copy; 2025 Deep Reinforcement Learning Course</p>
        <p><a href="../applications.html" style="color: var(--light);">Back to Applications</a> | <a href="../../index.html" style="color: var(--light);">Back to Home</a></p>
    </footer>
    
    <script src="static/classic-control.js"></script>
</body>
</html>